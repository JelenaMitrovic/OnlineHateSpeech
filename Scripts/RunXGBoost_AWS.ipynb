{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run XGBoost on AWS to speed things along\n",
    "##### adapted from run_xgboost_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report, auc\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import snowball, porter, wordnet\n",
    "#SnowballStemmer, porter.PorterStemmer, wordnet.WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename = '../Data/labeledhate_5cats.p'):\n",
    "    '''\n",
    "    Load data into a data frame for use in running model\n",
    "    '''\n",
    "    return pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitdata(df, classes, test_size=0.3):\n",
    "    '''\n",
    "    Split data into test & train; binarizes y into 5 classes\n",
    "    Outputs: X_train, X_test, y_train, y_test\n",
    "    '''\n",
    "    X = df.body\n",
    "    y = df.label\n",
    "\n",
    "    #relabel y labels to reflect integers [0-4] for xgboost\n",
    "    for ind in range(len(classes)):\n",
    "        y[(y==classes[ind])] = int(ind)\n",
    "\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed=[]\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizer(vectchoice = 'Count', stopwords = 'english', tokenize_me = None, max_features=500):\n",
    "    '''\n",
    "    Choose/return sklearn vectorizer, from Count Vectorizer, TFIDF, HashingVectorizer\n",
    "    Choose from: stopwords: ['english' or 'None'],\n",
    "                vectorizer = ['Count', 'Hash' or 'Tfidf']\n",
    "                tokenize_me: [None or tokenize]\n",
    "    '''\n",
    "\n",
    "    if vectchoice == 'Count':\n",
    "        vect = CountVectorizer(stop_words=stopwords, decode_error='ignore',\n",
    "                                tokenizer = tokenize_me, max_features=max_features)\n",
    "\n",
    "    # class sklearn.feature_extraction.text.CountVectorizer(input='content', encoding='utf-8',\n",
    "    # decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None,\n",
    "    # stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word',\n",
    "    # max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False,\n",
    "    # dtype=<class 'numpy.int64'>)[source]¶\n",
    "\n",
    "\n",
    "\n",
    "    elif vectchoice == \"Hash\":\n",
    "        vect = HashingVectorizer(stop_words=stopwords, decode_error = 'ignore',\n",
    "                                    non_negative=True, tokenizer = tokenize_me)\n",
    "\n",
    "    # class sklearn.feature_extraction.text.HashingVectorizer(input='content', encoding='utf-8',\n",
    "    # decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None,\n",
    "    #  stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word',\n",
    "    #  n_features=1048576, binary=False, norm='l2', non_negative=False, dtype=<class 'numpy.float64'>)[source]\n",
    "\n",
    "\n",
    "\n",
    "    elif vectchoice == 'Tfidf':\n",
    "        vect = TfidfVectorizer(stop_words=stopwords, decode_error='ignore',\n",
    "                                tokenizer = tokenize_me,max_features=max_features)\n",
    "\n",
    "\n",
    "    # class sklearn.feature_extraction.text.TfidfVectorizer(input='content', encoding='utf-8',\n",
    "    # decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None,\n",
    "    # analyzer='word', stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1),\n",
    "    # max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>\n",
    "    # , norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)[source]¶\n",
    "\n",
    "\n",
    "    # fit & transform train vector\n",
    "    vectfit_train = vect.fit_transform(X_train)\n",
    "    # transform test vector\n",
    "    vectfit_test = vect.transform(X_test)\n",
    "\n",
    "    return vectfit_train, vectfit_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_scores(y_test, y_score, y_preds):\n",
    "    '''\n",
    "    Print model performance stats.\n",
    "    y_test: test labels (sklearn format); y_score: probabilities; y_preds: predictions\n",
    "    '''\n",
    "    print(\"ROC AUC Score: {0}\".format(roc_auc_score(y_test, y_score)))\n",
    "    # print(\"Classification report: {0}\".format(classification_report(y_test, y_preds)))\n",
    "\n",
    "def top_features(d, n=20):\n",
    "    '''\n",
    "    Function to show the top n important features & their scores.\n",
    "    The get_fscore method in xgboost returns a dictionary of features & a number.\n",
    "    Get the top n features with the highest scores\n",
    "\n",
    "    d is a dictionary (from get_fscore method in xgboost)\n",
    "    '''\n",
    "\n",
    "    featureslist = []\n",
    "\n",
    "    for k, v in sorted(d.iteritems(), reverse=True, key=lambda (k,v): (v,k)):\n",
    "        featureslist.append((k,v))\n",
    "\n",
    "    return featureslist[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createmulticlassROC(classes, y_test, y_score):\n",
    "    '''\n",
    "    Function to create & plot ROC curve & associated areas\n",
    "    Adapted from http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "\n",
    "    Inputs: classes: a list of classes\n",
    "            y_test: the test labels, binarized into columns\n",
    "            y_score: the predicted probabilities for each class.\n",
    "                (e.g. y_score = classifier.fit(countv_fit_X_train, y_train).predict_proba(countv_fit_X_test) )\n",
    "    '''\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    n_classes = len(classes)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    # plt.figure(figsize = (12,8))\n",
    "    plt.figure()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                       ''.format(classes[i], roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for multi-class')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main block equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Splitting Data\n",
      "For vect Count, stemmer <nltk.stem.snowball.SnowballStemmer object at 0x109d30110> & token <function tokenize at 0x109ce8b18>\n",
      "Vectorizing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fbd77e0dcf66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Vectorizing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             vectfit_X_train, vectfit_X_test = vectorizer(vectchoice = vect,\n\u001b[0;32m---> 28\u001b[0;31m                         stopwords = 'english', tokenize_me = token, max_features=fnum)\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Classifying'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mxg_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectfit_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5160cf1e1e3c>\u001b[0m in \u001b[0;36mvectorizer\u001b[0;34m(vectchoice, stopwords, tokenize_me, max_features)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# fit & transform train vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mvectfit_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;31m# transform test vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mvectfit_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emily/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 817\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emily/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emily/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 238\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fe32df00e37f>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mstems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstem_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fe32df00e37f>\u001b[0m in \u001b[0;36mstem_tokens\u001b[0;34m(tokens, stemmer)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mstemmed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mstemmed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstemmed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emily/anaconda/lib/python2.7/site-packages/nltk/stem/snowball.pyc\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;31m# STEP 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__step2_suffixes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tional\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classes = ['NotHate', 'SizeHate', 'GenderHate', 'RaceHate', 'ReligionHate']\n",
    "\n",
    "print(\"Loading Data\")\n",
    "df = load_data()\n",
    "print(\"Splitting Data\")\n",
    "X_train, X_test, y_train, y_test = splitdata(df, classes)\n",
    "\n",
    "#relabel the output for multiclass roc plot, score\n",
    "ylabel_bin = label_binarize(y_test.astype(int), classes=[0,1,2,3,4],sparse_output=False)\n",
    "\n",
    "### Loop through # max_features? --> Use 5000 as a starting point, at least for now. ###\n",
    "### Use english stop words\n",
    "\n",
    "vect_options = ['Count', 'Hash', 'Tfidf']\n",
    "\n",
    "# token_options = [None, tokenize]\n",
    "token_options = [tokenize]\n",
    "\n",
    "stemmer = snowball.SnowballStemmer(\"english\")\n",
    "n_max_features = [500, 1000, 2000, 5000, 10000]\n",
    "\n",
    "for token in token_options:\n",
    "    for fnum in n_max_features:\n",
    "        for vect in vect_options:\n",
    "            print('For vect {0}, stemmer {1} & token {2}'.format(vect, stemmer, token))\n",
    "            print('Vectorizing')\n",
    "            vectfit_X_train, vectfit_X_test = vectorizer(vectchoice = vect,\n",
    "                        stopwords = 'english', tokenize_me = token, max_features=fnum)\n",
    "            print('Classifying')\n",
    "            xg_train = xgb.DMatrix(vectfit_X_train, label=y_train)\n",
    "            xg_test = xgb.DMatrix(vectfit_X_test, label=y_test)\n",
    "            # Set up xboost parameters\n",
    "            param = {}\n",
    "            # use softmax multi-class classification\n",
    "            # param['objective'] = 'multi:softmax'\n",
    "            # scale weight of positive examples\n",
    "            param['eta'] = 0.9\n",
    "            param['max_depth'] = 6\n",
    "            param['silent'] = 1\n",
    "            param['nthread'] = 4\n",
    "            param['num_class'] = 5\n",
    "\n",
    "            watchlist = [ (xg_train, 'train'), (xg_test, 'test') ]\n",
    "            num_round = 5\n",
    "            # bst = xgb.train(param, xg_train, num_round, watchlist )\n",
    "            param['objective'] = 'multi:softprob'\n",
    "            bst = xgb.train(param, xg_train, num_round, watchlist );\n",
    "            # get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "            yprob = bst.predict(xg_test).reshape(y_test.shape[0], 5)\n",
    "            ylabel = np.argmax(yprob, axis=1)\n",
    "\n",
    "            print('predicting, classification error=%f' % (sum( int(ylabel[i]) != y_test.iloc[i] for i in range(len(y_test))) / float(len(y_test)) ))\n",
    "            createmulticlassROC(classes, ylabel_bin, yprob)\n",
    "            print(\"ROC AUC Score: {0}\".format(roc_auc_score(ylabel_bin, yprob)))\n",
    "\n",
    "#             top_features(bst.get_fscore(), n=20)\n",
    "            print(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
